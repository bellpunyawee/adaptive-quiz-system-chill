# Adaptive Quiz System

Intelligent adaptive learning platform with **Bayesian-optimized Contextual Bandit** + **3PL IRT** for personalized question selection and AI-powered feedback.

**Status**: ‚úÖ Production Ready | **Version**: 4.0 (Research-Validated) | **Tests**: 158 passing | **Validation**: 14/14 targets met

---

## üéØ What It Does

Delivers personalized adaptive quizzes that:

- **Select optimal questions** using LinUCB contextual bandit + IRT hybrid
- **Estimate ability** in real-time with 3PL Item Response Theory
- **Provide AI feedback** with Google Gemini for wrong answers
- **Adapt learning paths** based on 15D context vectors per student

**Result**: Students reach mastery faster with better accuracy and higher engagement.

---

## ‚ö° Quick Start

```bash
# Install
npm install
npx prisma generate && npx prisma db push

# Configure
cp .env.example .env.local
# Add: DATABASE_URL, AUTH_SECRET, GEMINI_API_KEY

# Run
npm run dev
# Visit http://localhost:3000
```

**Admin Setup**: See [docs/QUICK_START_ADMIN.md](docs/QUICK_START_ADMIN.md)

**Full Guide**: See [docs/USER_GUIDE.md](docs/USER_GUIDE.md)

---

## üöÄ Key Features

### Intelligent Question Selection System

**What it does:**

- Adapts to each student's ability level in real-time
- Combines **LinUCB Contextual Bandit** (learns patterns) with **3PL IRT** (proven psychometric model)
- Uses 4,100 carefully curated questions (2,500 from ASSISTments public dataset + 1,600 strategically generated)

**Why it matters:**

Students learn faster because the system always asks questions at the right difficulty‚Äînot too hard (frustrating), not too easy (boring).

**How it works:**

1. **Learns your patterns** - Tracks 15 different aspects of how you learn (speed, accuracy, topic strengths)
2. **Selects optimal questions** - Picks questions that maximize learning while minimizing wasted time
3. **Adapts in real-time** - Adjusts difficulty as you improve or struggle

### System Validation (Tested with 1,000 Simulated Students)

**What we measured:**

| Metric | Result | What This Means |
|--------|--------|-----------------|
| **Actionable Precision** | 95.2% | 95 out of 100 students get accurate assessments (good enough to recommend next topics) |
| **System Health** | 22.4% rescue rate | Only 22% of questions needed backup selection (healthy content pool) |
| **Reliability** | 0% crash rate | No students got stuck or ran out of questions |
| **Efficiency** | 19.4 questions avg | Students reach accurate assessment in ~20 questions (vs 25+ baseline) |

**Interpretation for general audience:**

- ‚úÖ **Accurate** - The system correctly identifies your skill level 95% of the time
- ‚úÖ **Reliable** - Never runs out of appropriate questions for your level
- ‚úÖ **Efficient** - Gets accurate results with fewer questions (saves time)

### Question Pool Quality

**What we improved:**

| Difficulty Level | Before | After | Impact |
|------------------|--------|-------|--------|
| **Easy Questions** (for struggling students) | 5.7% | 13.0% | +128% more support for beginners |
| **Hard Questions** (for advanced students) | 4.2% | 16.3% | +288% more challenge for top performers |
| **High-Quality Questions** (clear skill measurement) | 18.9% | 55.8% | +195% better measurement precision |

**Interpretation:**
Before, the system struggled to find appropriate questions for beginners and advanced students. Now, every student gets questions matched to their level.

### Additional Features

- **AI-Powered Feedback** - Gemini 2.5 Flash with personalized explanations
- **Exposure Control** - Sympson-Hetter prevents over-use
- **Real-time Analytics** - Personalization metrics and monitoring
- **A/B Testing Ready** - Feature flags for gradual rollout

---

## üìä Research Validation & Performance

### Real-World Operational Metrics

**What we tested:** System performance with 1,000 simulated students across all skill levels

| Metric | Result | Target | Interpretation |
|--------|--------|--------|----------------|
| **Actionable Precision** | 95.2% | >90% | 95 out of 100 students get reliable skill assessments |
| **System Health** | 22.4% | <30% | Content pool is healthy (78% of selections are first-choice) |
| **Reliability** | 0% crashes | <1% | Zero students got stuck without appropriate questions |
| **Efficiency** | 19.4 questions | <25 | Average quiz length (vs 25+ for traditional methods) |

**Why this matters:** These metrics prove the system works reliably in real-world conditions, not just in theory.

**How we measure these:**

- **Actionable Precision** = % of students achieving SEM < 0.50 (where SEM = 1/‚àöI(Œ∏), I = Fisher Information)
- **System Health (Rescue Rate)** = (Questions needing fallback / Total questions) √ó 100%
- **Reliability (Crash Rate)** = (Students hitting hard stop / Total students) √ó 100%
- **Efficiency** = Average number of questions until convergence (SEM < target threshold)

**Data requirements:** Quiz sessions with student responses, ability estimates (Œ∏), and SEM at each question.

*Full formulations:* See [METRICS_REFERENCE.md](docs/METRICS_REFERENCE.md) for complete mathematical definitions.

### Research-Grade Accuracy Metrics

**What we measured:** Scientific validation using 5-fold cross-validation (gold standard in machine learning)

| Metric | Result | Target | What This Measures |
|--------|--------|--------|-------------------|
| **RMSE** | 0.401 | <0.50 | How close we estimate your true ability (lower = better) |
| **Calibration (ECE)** | 0.038 | <0.05 | How well our confidence matches reality (lower = better) |
| **Ranking (Kendall's Tau)** | 0.823 | >0.70 | How accurately we rank students by skill (higher = better) |
| **Prediction (Brier Score)** | 0.152 | <0.20 | How well we predict right/wrong answers (lower = better) |
| **Recommendation (NDCG@10)** | 0.891 | >0.80 | How good we are at recommending next questions (higher = better) |

**Interpretation for general audience:**

- **RMSE 0.401** - Our ability estimates are within ¬±0.4 points of true skill (on a -3 to +3 scale). That's like estimating someone's height within 2 inches.
- **ECE 0.038** - When we say "80% confident", we're correct 77-83% of the time (very well calibrated).
- **Kendall's Tau 0.823** - If we rank 100 students, we get 82 out of 100 pairs in the correct order.

**How we calculate these:**

- **RMSE** = ‚àö(Œ£(Œ∏ÃÇ·µ¢ - Œ∏·µ¢)¬≤ / n) ‚Äî Root mean squared error between estimated and true ability
- **ECE** = Œ£|accuracy(bin) - confidence(bin)| √ó |bin| / n ‚Äî Expected calibration error across probability bins
- **Kendall's Tau** = (concordant pairs - discordant pairs) / total pairs ‚Äî Rank correlation coefficient
- **Brier Score** = Œ£(predicted_prob - actual_outcome)¬≤ / n ‚Äî Mean squared error of probability predictions
- **NDCG@10** = DCG@10 / IDCG@10 ‚Äî Normalized discounted cumulative gain for top-10 recommendations

**Data requirements:** Cross-validation with known true abilities (Œ∏), predicted probabilities, and actual outcomes.

*Full formulations with examples:* [METRICS_REFERENCE.md](docs/METRICS_REFERENCE.md)

### Personalization Quality

**What we measured:** How well the system adapts to individual students

| Metric | Result | What This Means |
|--------|--------|-----------------|
| **Question Diversity** | 35.36 | Each student sees a personalized mix of questions (not the same test for everyone) |
| **Context Features** | 15 dimensions | System considers 15 different aspects of each student's learning pattern |
| **Selection Balance** | 0.467 | Questions are well-distributed (no over-reliance on a few "favorite" questions) |

**Interpretation:** Two students at the same skill level will see different questions based on their individual learning patterns‚Äîtrue personalization, not just difficulty matching.

**How we measure these:**

- **Question Diversity** = Average count of unique questions per student (not a percentage) ‚Äî Measures exploration breadth
- **Context Features** = Dimensionality of feature vector (15D: user √ó question √ó interaction features)
- **Selection Balance (Gini)** = 1 - Œ£(p·µ¢¬≤) where p·µ¢ = proportion of times question i was selected ‚Äî Lower = more balanced

**Data requirements:** Complete question selection logs across all students, feature vectors, selection frequencies.

**Data source:** Question Diversity (35.36) from `monte-carlo-contextual-bandit.ts` comparison simulations. Selection Balance (0.467) requires recalculation with Phase 4 data.

*Implementation details:* See `scripts/testing/monte-carlo-contextual-bandit.ts` lines 863-867 for diversity calculation.

---

**Bottom line:** All 14 validation targets met. System is both scientifically rigorous and operationally reliable.

**Full technical details:** [Complete Results Report](docs/VALIDATION_REPORT.md) | [Reproduction Guide](docs/PUBLICATION_PIPELINE.md)

---

## üõ†Ô∏è Tech Stack

- **Framework**: Next.js 15 (App Router) + TypeScript
- **Database**: SQLite + Prisma ORM (+ MongoDB)
- **Authentication**: NextAuth.js v5
- **AI**: Gemini 2.5 Flash (Dependable)
- **Key Algorithms**: LinUCB (contextual bandit), 3-PL IRT, Bayesian optimization

---

## üìö Documentation

### Getting Started

| For... | See... |
|--------|--------|
| **Quick Start** | [docs/USER_GUIDE.md](docs/USER_GUIDE.md) |
| **Admin Setup** | [docs/QUICK_START_ADMIN.md](docs/QUICK_START_ADMIN.md) |
| **Environment Setup** | [docs/ENVIRONMENT_SETUP.md](docs/ENVIRONMENT_SETUP.md) |

### Research & Publication

| For... | See... |
|--------|--------|
| **üìä Publication Pipeline** | [docs/PUBLICATION_PIPELINE.md](docs/PUBLICATION_PIPELINE.md) ‚≠ê |
| **üìà Metrics Reference** | [docs/METRICS_REFERENCE.md](docs/METRICS_REFERENCE.md) ‚≠ê |
| **Validation Results** | [docs/VALIDATION_REPORT.md](docs/VALIDATION_REPORT.md) |
| **Phase 3 Baseline** | [docs/archive/PHASE3_VALIDATION_REPORT.md](docs/archive/PHASE3_VALIDATION_REPORT.md) |
| **Operational Philosophy** | [docs/OPERATIONAL_RELIABILITY.md](docs/OPERATIONAL_RELIABILITY.md) ‚≠ê |
| **Assessment Contexts** | [docs/FORMATIVE_VS_SUMMATIVE.md](docs/FORMATIVE_VS_SUMMATIVE.md) ‚≠ê |

### Technical Guides

| For... | See... |
|--------|--------|
| **Contextual Bandit** | [docs/CONTEXTUAL_BANDIT_GUIDE.md](docs/CONTEXTUAL_BANDIT_GUIDE.md) |
| **3PL IRT Technical** | [docs/3PL_COMPLETE_GUIDE.md](docs/3PL_COMPLETE_GUIDE.md) |
| **Convergence Config** | [docs/CONVERGENCE_CONFIG_GUIDE.md](docs/CONVERGENCE_CONFIG_GUIDE.md) |
| **Personalization Metrics** | [docs/PERSONALIZATION_METRICS_GUIDE.md](docs/PERSONALIZATION_METRICS_GUIDE.md) |
| **Testing & Evaluation** | [docs/SIMULATION_EVALUATION_GUIDE.md](docs/SIMULATION_EVALUATION_GUIDE.md) |

‚≠ê = New validation documentation (Dec 2025)

---

## Running Tests & Evaluation

### Test Suite

```bash
npm test                # All 158 tests
npm run build          # Production build check
```

### Research Reproduction Pipeline (For Researchers & Peer Review)

**What it does:** Complete automated reproduction of all research results from scratch (2-4 hours)

```bash
# Full automated pipeline
./scripts/testing/test-publication-pipeline.sh

# Or Windows:
scripts\testing\test-publication-pipeline.bat
```

**Why it exists:** Enables independent researchers to validate our findings and reproduce results for peer review.

**How it works (4 phases):**

1. **Phase 0: Data Preparation** (15-30 min) - Validate ASSISTments dataset (2,500 questions), validate Phase 4 generated questions (1,600 questions), analyze pool distribution (13% easy, 16% hard)
2. **Phase 1: Core Operational Validation** (60-90 min) - Monte Carlo Phase 3 simulation (1,000 students √ó 3 runs), algorithm comparison (Hybrid vs IRT vs LinUCB), operational metrics (precision, rescue, crash)
3. **Phase 2: Research-Grade Validation** (45-60 min) - Cross-validation (5-fold, 200+ students), baseline comparison (6 algorithms), statistical significance tests
4. **Phase 3: Analysis & Reporting** (15-20 min) - Generate publication report (2-page + 15-page), create 8 visualization figures, statistical comparison tables

**Outputs:**

- Executive summary (2 pages) - Quick overview of results
- Complete technical report (15 pages) - All metrics, tables, statistical tests
- Publication figures (8 images) - Charts, calibration plots, rescue funnels

### Individual Scripts

```bash
# Primary simulation (1,000 synthetic students)
npx tsx scripts/testing/monte-carlo-phase3.ts

# Algorithm comparison (Hybrid vs IRT vs LinUCB)
npx tsx scripts/testing/monte-carlo-contextual-bandit.ts

# Cross-validation (5-fold, progressive checkpoints)
npx tsx scripts/testing/cross-validation.ts

# Baseline comparison (6 algorithms)
npx tsx scripts/testing/baseline-models.ts
```

See [scripts/README.md](scripts/README.md) and [docs/PUBLICATION_PIPELINE.md](docs/PUBLICATION_PIPELINE.md) for details.

---

## HOW IT WORKS

### 1. Contextual Bandit (LinUCB)

Learns from 15D context vectors:

- **User features**: ability, response patterns, session progress
- **Question features**: difficulty, discrimination, topic
- **Interaction features**: IRT probability, topic weakness match

**Algorithm**: Upper Confidence Bound with linear regression

```
UCB(x) = x^T Œ∏ÃÇ + Œ±‚àö(x^T A^(-1) x)
```

### 2. 3PL IRT Model

Accounts for guessing in multiple-choice:

```
P(Œ∏) = c + (1-c) / (1 + exp(-a(Œ∏-b)))
```

- `a` = discrimination (0.8-2.2)
- `b` = difficulty (-2.0 to +2.0)
- `c` = guessing (0.15-0.30)

### 3. Hybrid Scoring

Combines LinUCB with IRT using Bayesian-optimized weights:

```
Score = w_linucb √ó UCB_linucb + w_irt √ó UCB_irt
```

Weights evolve from 40% ‚Üí 97% LinUCB as confidence grows.

---

##  Recent Updates

### December 3, 2025 - Content Expansion & Research Validation

**What we accomplished:**

- ‚úÖ **Expanded question pool** - Added 1,600 strategically generated questions to fill gaps
- ‚úÖ **4,100 total questions** - 2,500 from public dataset + 1,600 custom (128% more easy, 288% more hard)
- ‚úÖ **Validated operational reliability** - 95.2% precision, 0% crash rate, all targets met
- ‚úÖ **Publication-ready pipeline** - Complete 2-4 hour reproduction guide for researchers
- ‚úÖ **Cleaned codebase** - 70% file reduction (19 docs, 22 scripts) for maintainability

**Why this matters:**

Students no longer experience content gaps. Beginners get appropriate support, advanced students get sufficient challenge, and the system never runs out of questions.

**Research shift:** Moved from theoretical "fairness gap" metric to practical "operational reliability"‚Äîfocusing on what matters for real learning (Can we accurately guide each student?) rather than statistical equality.

### November 2025 - Intelligent Personalization Engine

**What we built:**

- ‚úÖ **LinUCB contextual bandit** - Learns individual student patterns (~3,000 lines of code)
- ‚úÖ **15-dimensional personalization** - Tracks speed, accuracy, topic strengths, and 12 other factors
- ‚úÖ **Optimized hybrid algorithm** - Bayesian optimization to find best balance (38 iterations)
- ‚úÖ **Multi-objective learning** - Balances correctness, information gain, and time efficiency

**Output:** System adapts to each student's unique learning style, not just their skill level.

### Earlier - Core Foundation

**What we established:**

- ‚úÖ **3PL IRT psychometric model** - Proven mathematical framework for skill assessment
- ‚úÖ **AI-powered explanations** - Gemini-generated personalized feedback for wrong answers
- ‚úÖ **Comprehensive testing** - 158 automated tests ensure reliability

---

## üìÅ Project Structure

```
adaptive-quiz-system/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/                      # Next.js 15 app router
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contextual-bandit/   # LinUCB implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid.ts        # ‚òÖ Optimized parameters
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ algorithms/      # LinUCB, Sherman-Morrison
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ features/        # 15D context vectors
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring.ts    # Analytics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adaptive-engine/     # 3PL IRT core
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ irt-3pl.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine-enhanced.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stopping-criteria.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ research/            # Research metrics
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ metrics.ts
‚îú‚îÄ‚îÄ scripts/                     # ‚òÖ Publication pipeline (22 files)
‚îÇ   ‚îú‚îÄ‚îÄ admin/                   # System administration (3)
‚îÇ   ‚îú‚îÄ‚îÄ external-data/           # Dataset integration (2)
‚îÇ   ‚îî‚îÄ‚îÄ testing/                 # Publication scripts (15)
‚îÇ       ‚îú‚îÄ‚îÄ archive/             # Historical baselines (2)
‚îÇ       ‚îú‚îÄ‚îÄ results/             # Simulation outputs
‚îÇ       ‚îî‚îÄ‚îÄ figures/             # Publication visualizations
‚îú‚îÄ‚îÄ docs/                        # ‚òÖ 23 documentation files (research-validated)
‚îÇ   ‚îú‚îÄ‚îÄ PUBLICATION_PIPELINE.md  # ‚≠ê Research reproduction guide
‚îÇ   ‚îú‚îÄ‚îÄ METRICS_REFERENCE.md     # ‚≠ê Complete metrics dictionary
‚îÇ   ‚îú‚îÄ‚îÄ OPERATIONAL_RELIABILITY.md  # ‚≠ê Design philosophy
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ prisma/                      # Database schema + 4,100 questions
```

‚òÖ = Recently streamlined (Dec 2025): 70% file reduction, clearer organization

---

## Configuration

### Basic Setup (.env.local)

```bash
# Required
DATABASE_URL="file:./prisma/dev.db"
AUTH_SECRET="<openssl rand -base64 32>"
NEXTAUTH_URL="http://localhost:3000"
GEMINI_API_KEY="<from https://aistudio.google.com/apikey>"

# Contextual Bandit (optional)
CONTEXTUAL_BANDIT_ENABLED=false      # Enable/disable
CONTEXTUAL_BANDIT_MODE=hybrid        # hybrid | linucb | irt-only
CONTEXTUAL_BANDIT_TRAFFIC=0          # 0-100 (% of users)
```

See [docs/ENVIRONMENT_SETUP.md](docs/ENVIRONMENT_SETUP.md) for complete guide.

---

## For Team Members

### Quick Reference

- **Set up locally** ‚Üí [docs/USER_GUIDE.md](docs/USER_GUIDE.md#quick-start)
- **Understand the algorithms** ‚Üí [docs/CONTEXTUAL_BANDIT_GUIDE.md](docs/CONTEXTUAL_BANDIT_GUIDE.md)
- **Reproduce research** ‚Üí [docs/PUBLICATION_PIPELINE.md](docs/PUBLICATION_PIPELINE.md) ‚≠ê NEW
- **Understand metrics** ‚Üí [docs/METRICS_REFERENCE.md](docs/METRICS_REFERENCE.md) ‚≠ê NEW
- **See validation results** ‚Üí [docs/VALIDATION_REPORT.md](docs/VALIDATION_REPORT.md)
- **Run tests** ‚Üí `npm test` or `./scripts/testing/test-publication-pipeline.sh`
- **Use scripts** ‚Üí [scripts/README.md](scripts/README.md)
- **Deploy to production** ‚Üí [docs/ENVIRONMENT_SETUP.md](docs/ENVIRONMENT_SETUP.md)
- **Report issues** ‚Üí Create GitHub issue with reproduction steps

### Key Files to Know

- `src/lib/contextual-bandit/hybrid.ts` - **Core algorithm** (Bayesian-optimized weights)
- `src/lib/adaptive-engine/stopping-criteria.ts` - **Convergence logic** (skill-based stopping rules)
- `docs/PUBLICATION_PIPELINE.md` - **Research reproduction guide** ‚≠ê (Dec 3)
- `docs/METRICS_REFERENCE.md` - **Complete metrics dictionary** ‚≠ê (Dec 3)
- `docs/VALIDATION_REPORT.md` - **Validation results** (14/14 targets met)
- `scripts/testing/monte-carlo-phase3.ts` - **Primary simulation** (1,000 students)

---

<!-- ## üìà What's Next

### Short-term

- [ ] A/B testing framework for production validation
- [ ] Real-time dashboard for monitoring
- [ ] Scenario-specific optimization (Easy, Hard, Balanced)

### Medium-term

- [ ] Student-adaptive parameters (personalized evolution)
- [ ] Multi-objective optimization with constraints
- [ ] Advanced engagement analytics

### Long-term

- [ ] Deep learning for context embedding
- [ ] Federated learning for privacy-preserving personalization
- [ ] Multi-domain support

--- -->

## üìñ Research & References

**Key Papers**:

- Will be updated soon
<!-- - Li et al. (2010) - LinUCB for Contextual Bandits
- Birnbaum (1968) - 3PL IRT Model
- Lord (1980) - Fisher Information in Testing
- Mockus (1989) - Bayesian Optimization with Gaussian Process -->

**Implementation Scale**:

- ~15,000 lines of TypeScript (production code)
- 158 automated tests (ensuring reliability)
- 4,100 curated questions (2,500 from public dataset + 1,600 strategically generated)
- 15-dimensional personalization (tracking individual learning patterns)
- 22 research scripts (streamlined from 58‚Äî70% reduction for clarity)
- 23 documentation files (19 core + 4 new validation guides)

**Research Validation Completeness**:

- ‚úÖ All 14/14 operational + research targets met
- ‚úÖ Monte Carlo simulation (1,000 synthetic students, 3 independent runs)
- ‚úÖ Cross-validation (5-fold, gold standard methodology)
- ‚úÖ Baseline comparison (6 alternative algorithms tested)
- ‚úÖ Statistical significance (t-test, Wilcoxon, McNemar's, Bootstrap CI)
- ‚úÖ Publication-ready outputs (2-page summary + 15-page report + 8 figures)
- ‚úÖ Complete 2-4 hour reproduction pipeline (open science)

---

## üìù License

CHILL NUS

---

**Last Updated**: December 3, 2025

**Status**: ‚úÖ Production Ready - Research Validated - Publication Pipeline Ready

**Version**: 4.0 (Operational Reliability - Research-Validated)

**Contact**: See repository maintainers

---

**Quick Links**: [Reproduction Guide](docs/PUBLICATION_PIPELINE.md) | [Metrics Explained](docs/METRICS_REFERENCE.md) | [Complete Results](docs/VALIDATION_REPORT.md) | [All Scripts](scripts/README.md) | [Quick Start](docs/USER_GUIDE.md)
