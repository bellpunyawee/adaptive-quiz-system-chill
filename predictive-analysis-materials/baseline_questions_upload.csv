question_text,topic_name,option_1,option_2,option_3,option_4,option_5,correct_option,explanation,bloom_taxonomy,tags,image_filename
"Which of the following is/are of nominal data type? i. Hotel class from 1-star, 2-stars... to 5 stars where the higher number of stars indicate a more luxurious hotel ii. Ticket prices (in $) to different tourist attractions iii. Types of tourist attractions (art, adventure, nature, culture, etc.) iv. Location of attractions (e.g. Jurong, Mandai, Sentosa, Orchard, etc.) v. Distance from MRT for the tourist attractions (in meters)",Level of measurement,i only,"i, iii and iv only",ii and v only,iii only,iii and iv only,5,"Nominal data are categorical without inherent order. Types of attractions (iii) and Location (iv) are nominal. Hotel stars (i) is ordinal (has inherent order). Prices (ii) and Distance (v) are ratio/interval data.",,baseline,
"Suppose an analyst collected data from a supermarket on the daily sales (in dollars) of vegetables in the last year. His analysis produces: N=365, mean=$2350, standard deviation=$50, standard error=$2.617, Prediction Interval=($2251.54, $2448.46). Assume sales are relatively stable throughout the year. Which is a correct interpretation?",Confidence Interval,"We can predict with 95% confidence level that the sales of vegetables on a given day would be between $2251.54 to $2448.46","We can predict with 95% confidence level that the mean daily sales for vegetables will be between $2251.54 to $2448.46","We can predict that there is a 95% probability that the average daily sales for vegetables will lie between $2251.54 to $2448.46","We can be 95% sure that the average daily sales of vegetables would be between $2251.54 to $2448.46","We can predict there is a 95% chance that a randomly chosen sample mean will lie between $2251.54 and $2448.46",1,"A prediction interval predicts where a single future observation will fall, not the mean. Option (a) correctly interprets the prediction interval for an individual day's sales.",,baseline,
"A study was conducted on the 2020 cohort of graduates to gather their salary data. A survey was administered one year after graduation and again two years after graduation. Graduates who responded to both surveys were kept in the dataset. To compare if there is a significant increase in salary from year 1 to year 2, which test would be most appropriate?",Hypothesis Testing,Paired 2-sample t-test,Independent 2-sample t-test,Anova test,Paired z-test,One sample t-test,1,"Since the same individuals are measured at two time points, the data is paired/dependent. A paired t-test is appropriate for comparing means of two related groups.",,baseline,
"The table below shows descriptive statistics for Weekly Expenditure computed using the psych package: n=40, mean=328.82, sd=146.70, median=294.90, min=114.96, max=775.02, skew=2.025, kurtosis=0.994, se=23.19. Which best describes the distribution?",Probability Distributions,Highly left skewed with shorter and thinner tails than normal distribution,Highly positively skewed with longer and thicker tails than normal distribution,Moderately positively skewed with shorter and thinner tails than normal distribution,Relatively symmetrical with longer and thicker tails than normal distribution,Perfectly symmetrical and follows a normal distribution,2,"Skew=2.025 indicates highly positive (right) skew. Kurtosis=0.994 (excess kurtosis near 0 or positive) suggests tails similar to or heavier than normal distribution.",,baseline,
"Housing data modelled with Price ~ lotsize + bedrooms. Regression output shows: Intercept=5.613e+03 (p=0.172), lotsize=6.053e+00 (p<2e-16), bedrooms=1.057e+04 (p=2.31e-16). Which statements are correct? i. Adding more variables decreases overall R-Square ii. p-value <2e-16 means not much evidence for lotsize coefficient different from zero iii. p-value 2.31e-16 means evidence for bedrooms coefficient different from zero iv. For one more bedroom, average price increases by $5613 v. Fitted function: Price = 5613 + 6.053*lotsize + 10570*bedrooms",Regression Analysis,i and ii only,"ii, iii and v only","i, iii and v only",iii and v only,"i, ii, and v only",4,"Statement iii is correct (very small p-value = significant). Statement v shows the correct regression equation. Statement i is false (adding variables increases R-square). Statement ii is false (small p-value = strong evidence). Statement iv confuses intercept with bedroom coefficient.",,baseline,
"Multivariate regression for math10 ~ log(expend) + lnchprg. Variables: math10=percentage passing MEAP math, expend=expenditure per student, lnchprg=percentage in school lunch programme (linked to poverty). Coefficient for lnchprg=-0.30459 (p<2e-16). What is the correct interpretation?",Regression Analysis,"For every 1-percentage-point increase in lnchprg, predicted math10 decreases by 0.30%, holding other variables constant",Schools with higher lnchprg always have lower math10 regardless of all other factors,A 1% increase in lnchprg causes a 30% decline in math10,The relationship between lnchprg and math10 is not statistically significant at any reasonable confidence level,lnchprg explains all variation in math10 once log(expend) is controlled for,1,"The coefficient -0.30459 means for each 1-unit increase in lnchprg, predicted math10 decreases by 0.30 percentage points, holding other variables constant.",,baseline,
"99% confidence interval results for math10 regression: Intercept (-85.25, 44.53), log(expend) (-1.46, 13.92), lnchprg (-0.396, -0.213). Based on the 99% CI, which statement is correct?",Confidence Interval,"Because the 99% CI for lnchprg does not include zero, there is strong evidence that lnchprg is associated with lower math performance","Although the coefficient is negative, the wide confidence interval suggests the relationship is too uncertain","Since the 99% CI is narrow and fully above zero, lnchprg is a positive predictor of math10","The CI includes zero at the 99% level, so lnchprg is not statistically significant",The coefficient is significant only at the 95% level but not at the 99% level,1,"The 99% CI for lnchprg is entirely negative (-0.396 to -0.213), not including zero. This provides strong evidence of a significant negative association with math10.",,baseline,
"Histogram shows variable Z with frequencies: 1 at around -10000, 18 near 0, 72 in 0-10000 range, 6 around 10000, 2 in 10000-20000, and 1 outlier at 30000. Boxplot output shows outliers: 10668, 10733, -5577, 25405. Which conclusions are correct? i) Z has left skewed distribution ii) Z has one outlier based on histogram iii) Z has three outliers based on boxplot iv) Z has right skewed distribution v) Z is normally distributed",Probability Distributions,ii and iv only,ii and iii only,v only,"i, ii and iii only","ii, iii and iv only",1,"The histogram shows right skew (long tail to right). The boxplot shows 4 outlier values. Statement ii (one extreme outlier visible in histogram) and iv (right/positive skew) are correct.",,baseline,
"Given DataFrame store_sales, which code correctly calculates the mean of each row across columns 3 to 8?",Python Data Structures,store_sales.iloc[3:9].mean(axis=0),"store_sales.iloc[:, 3:9].mean(axis=0)","store_sales.iloc[:, 3:9].mean(axis=1)",store_sales.iloc[3:9].mean(axis=1),"store_sales.iloc[:, 3:9].mean()",3,"iloc[:,3:9] selects all rows and columns 3-8 (0-indexed). mean(axis=1) calculates mean across columns for each row.",,baseline,
"Dataset segment_data has columns Segment, own_home, and subscribe. You want frequency of Segment and own_home combinations showing True/False breakdown by segment. Which code produces this result?",Python Data Structures,"segment_data.groupby(['Segment', 'own_home'])['subscribe'].count().unstack()","segment_data.groupby(['Segment', 'own_home'])['subscribe'].sum().unstack()",segment_data['own_home'].value_counts().unstack(),"segment_data.pivot(index='Segment', columns='own_home', values='subscribe').count()","segment_data.groupby(['Segment', 'own_home'])['subscribe'].count().pivot_table(columns='own_home')",1,"groupby(['Segment','own_home']) groups by both columns, ['subscribe'].count() counts rows in each group, and unstack() pivots own_home values (True/False) into columns.",,baseline,
"Dataset Auto loaded with pd.read_csv('Auto.data', na_values=['?'], delim_whitespace=True). You want DataFrame with weight and origin columns for cars built after 1980 (year > 80). Which code achieves this?",Python Data Structures,"idx_80 = Auto['year'] > 1980; Auto_re = Auto.loc[idx_80, ['weight', 'origin']]","idx_80 = Auto['year'] > 80; Auto_re = Auto.loc[idx_80, ['weight', 'origin']]","idx_80 = Auto['year'] <= 80; Auto_re = Auto.loc[idx_80, ['weight', 'origin']]","idx_80 = Auto['year'] > 80; Auto_re.loc[idx_80, ['weight', 'origin']]","Auto_re = Auto[Auto['year'] > 80][['weight', 'origin']]",2,"The year column contains 2-digit years (e.g., 80 for 1980). Option b correctly filters year > 80 and assigns result to Auto_re.",,baseline,
"Consider student data: Sarah(poor,lots,Yes), Dana(average,some,No), Alex(average,some,No), Annie(average,lots,Yes), Emily(excellent,lots,Yes), Pete(excellent,lots,No), John(excellent,lots,No), Kathy(poor,some,No). Using Naive Bayes, compute Score(Hirable=Yes | GPA=poor, Effort=lots).",Probability Distributions,"Score(Yes) = P(Yes) × P(GPA=poor|Yes) × P(Effort=lots|Yes) = 3/8 × 1/3 × 1 = 1/8","Score(Yes) = P(No) × P(GPA=poor|No) × P(Effort=lots|No) = 5/8 × 1/5 × 2/5 = 1/20","Score(Yes) = (1/8)/(1/8 + 1/20) = 5/7","Score(Yes) = P(Yes) × P(GPA=poor|Yes) × P(Effort=lots|Yes) = 1/2 × 1/3 × 1 = 1/6","Score(Yes) = P(Yes) × P(GPA=poor|Yes) × P(Effort=lots|Yes) = 5/8 × 1/3 × 1 = 5/24",1,"P(Yes)=3/8, P(GPA=poor|Yes)=1/3 (1 out of 3 Yes cases), P(Effort=lots|Yes)=3/3=1. Score(Yes) = 3/8 × 1/3 × 1 = 1/8.",,baseline,
"An analyst has 5000 predictors and 200 samples. She selects 100 predictors most correlated with class labels using all samples, then performs 5-fold CV with these 100 predictors. CV error=3%, true test error=50%. Which explains this discrepancy? i. Predictors chosen using all samples caused test fold leakage ii. CV applied only after predictor selection gave optimistic estimate iii. Sample size too small to prevent overfitting iv. Classifier performs poorly with standardized data v. Predictors not filtered by variance",Regression Analysis,ii and iv only,iv only,v only,i and ii only,i only,4,"Both i and ii are correct. Using all samples (including test folds) for predictor selection causes information leakage. CV was applied only after this flawed selection, leading to overly optimistic error estimate.",,baseline,
"Check all binary classifiers that can correctly separate the training data (circles vs triangles) shown in figure (scattered points not linearly separable).",Machine Learning,Logistic regression,Support Vector Machines (SVM) with linear kernel,Support Vector Machines (SVM) with RBF kernel,Decision tree,3-nearest-neighbor classifier (with Euclidean distance),3,"The data points are not linearly separable. SVM with RBF kernel can create non-linear decision boundaries to separate them.",,baseline,
"In Bayesian learning, what does the posterior probability represent?",Probability Distributions,The probability of the observed data given the model parameters,The initial probability of the model before observing data,"The updated probability of a model, after having seen the data",The probability of the data marginalized over all parameter values,The probability of the model being true without considering data,3,"Posterior probability is the updated probability of a hypothesis/model after incorporating observed data. It combines prior probability with likelihood via Bayes' theorem.",,baseline,
"What are support vectors in the context of Support Vector Machines (SVM)?",Machine Learning,The training examples farthest from the decision boundary,The only training examples necessary to compute the decision function f(x) in an SVM,The class centroids (mean points of each class),The directions that maximize the separation between classes,All training examples that are correctly classified by the model,2,"Support vectors are training examples on or within the margin boundaries. They are the only points needed to define the decision boundary; removing other points wouldn't change the classifier.",,baseline,
"According to ensemble methods, which statement about bagging, random forests, and boosting is FALSE?",Machine Learning,Bagging reduces variance by averaging predictions from multiple trees fit to bootstrapped samples,Random forests improve upon bagging by decorrelating trees through feature randomization at each split,"Boosting builds trees sequentially, with each new tree trained on a modified version based on previous trees' residuals",Out-of-bag (OOB) error estimation is available for boosting but not for bagging or random forests,In bagging increasing the number of trees B generally does not lead to overfitting,4,"Statement d is FALSE. OOB error estimation is available for bagging and random forests, NOT for boosting. Each tree is trained on a bootstrap sample, leaving ~1/3 data out-of-bag for error estimation.",,baseline,
"Which statement about decision trees is FALSE?",Machine Learning,Regression trees predict using the mean response in each region,Gini index and entropy measure node purity for classification trees; RSS is for regression trees,"Cost-complexity pruning grows a large tree first, then prunes based on α",Recursive binary splitting is a bottom-up approach,Decision trees handle both quantitative and qualitative predictors,4,"Statement d is FALSE. Recursive binary splitting is a TOP-DOWN (greedy) approach, not bottom-up. It starts at the root and recursively partitions the data.",,baseline,
"Feature vectors {(-2,-2), (-2,0), (1,3), (2,2), (3,4)}. Apply greedy agglomerative hierarchical clustering using centroid linkage. Two visualizations provided (cluster diagrams A,B and dendrograms C,D). Which option shows the correct pair for centroid linkage?",Machine Learning,A and C,A and D,B and C,B and D,None of the pairs,3,"Using centroid linkage, clustering proceeds by merging clusters whose centroids are closest. Diagram B and dendrogram C correctly represent this process.",,baseline,
"Consider the dendrogram with items BA, NA, RM, FI, MI, TO. Using this dendrogram to create 3 clusters, what would the clusters be?",Machine Learning,"{BA, NA}, {RM, FI}, {MI, TO}","{NA, RM}, {BA, FI}, {MI, TO}","{BA, NA, RM, FI}, {MI}, {TO}","{BA, NA, RM}, {FI}, {MI, TO}",None of these,1,"Cutting the dendrogram at the appropriate height to get 3 clusters gives: {BA,NA}, {RM,FI}, and {MI,TO}.",,baseline,
"In simple exponential smoothing forecasting, to give higher weightage to recent demand information, the smoothing constant must be close to:",Forecasting,-1,Zero,0.5,1,None of the above,4,"In exponential smoothing, alpha (smoothing constant) ranges from 0 to 1. Higher alpha gives more weight to recent observations. Alpha close to 1 gives maximum weight to the most recent data.",,baseline,
"For a hotel, actual demand for disposable cups was 600 units in January and 700 units in February. The forecast for January was 500 units. Using simple exponential smoothing (alpha=0.8), what is the forecast for March?",Forecasting,676,576,680,580,612,1,"Feb forecast = 500 + 0.8*(600-500) = 500 + 80 = 580. March forecast = 580 + 0.8*(700-580) = 580 + 96 = 676.",,baseline,
